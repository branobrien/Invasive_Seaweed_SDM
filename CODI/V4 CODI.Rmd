---
title: "V4 CODI"
author: "Brandon O'Brien"
date: "1/21/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = '~/Documents/Algae Lab UNH/Species Distribution Modeling/V4 - Jan 2022')
```

```{r library}

setwd("~/Documents/Algae Lab UNH/Species Distribution Modeling/V4 - Jan 2022")

library(ggplot2)
library(ggspatial)
library(sf)
library(rnaturalearth)
library(rnaturalearthdata)
library(raster)
library(dismo)
library(rJava)
library(usdm)
library(terra)


```

```{r occur data}

# note - maxent function won't take the raw occurrence data, it wants a dataframe with only 2 columns.
# Also, needs to be in Long, Latt order or else it messes up.

occur.codi <- read.csv('CODI/CODI Occur/codi_clip_max.csv', header=T)
head(occur.codi)
occur.codi <- as.data.frame(occur.codi)

```

```{r occur maps}

world <- ne_countries(scale='medium', returnclass='sf')


theme_map <- theme(panel.background = element_rect(color = 'black', fill = 'lightblue'),
                   panel.border = element_rect(color = 'black', fill=NA))
# global
ggplot(data=world) +
  geom_sf(color='black', fill='gray') + 
  geom_point(data=occur.codi, aes(x=longitude, y=latitude), 
             color='red', fill='red', size=1)+
  labs(caption = 'Figure 1: Global occurrence records of Codium fragile subsp. fragile.',
      x = '', y = '') +
  coord_sf(expand = F) +
  theme_map



```




```{r envi data present-day}



# bringing in all environmental raster layers and getting them  trimmed and set up
# building raster'bricks' which are just collections of layers in same extent
# bricks are like stacks but apparently faster to work with.


## all envi layers
## these have been trimmed down in arcGIS to only the 100km belt along the coastlines

SST.max <- raster('Environmental Data/Clipped_100km/e08.SST.max.asc')
SST.min <- raster('Environmental Data/Clipped_100km/e09.SST.min.asc')
SST.ran <- raster('Environmental Data/Clipped_100km/e10.SST.ra.asc')
SAL.men <- raster('Environmental Data/Clipped_100km/e07.SAL.me.asc')
NIT.max <- raster('Environmental Data/Clipped_100km/e03.NIT.max.asc')
NIT.min <- raster('Environmental Data/Clipped_100km/e04.NIT.min.asc')
PHO.max <- raster('Environmental Data/Clipped_100km/e05.PHO.max.asc')
PHO.min <- raster('Environmental Data/Clipped_100km/e06.PHO.min.asc')
DA.max <- raster('Environmental Data/Clipped_100km/e01.DA.max.asc')
DA.min <- raster('Environmental Data/Clipped_100km/e02.DA.min.asc')



## NOtes
# Brezo had a lot to say about using DA as a variable - maybe I should just remove it from the start?
# Also said it may be worth it to include all the temp vars, max and min
# even if somewhat correlated

# need to trim off edges to make sure extents are all the same

ext.global.trim <- extent(c(-180,180,-65,65))

SST.max <- crop(SST.max, ext.global.trim)
SST.min <- crop(SST.min, ext.global.trim)
SST.ran <- crop(SST.ran, ext.global.trim)
SAL.men <- crop(SAL.men, ext.global.trim)
NIT.max <- crop(NIT.max, ext.global.trim)
NIT.min <- crop(NIT.min, ext.global.trim)
PHO.max <- crop(PHO.max, ext.global.trim)
PHO.min <- crop(PHO.min, ext.global.trim)
DA.max <- crop(DA.max, ext.global.trim)
DA.min <- crop(DA.min, ext.global.trim)

#not using DA on Brezo's suggestions

envi.global <- stack(c(SST.max,
                       SST.min,
                       SST.ran,
                       SAL.men,
                       NIT.max,
                       NIT.min,
                       PHO.max,
                       PHO.min))
                       # DA.max,
                       # DA.min))

```

```{r collinearity tests}

## this entire section is #d out since I don't need to rerun it during knitting (analysis done though)

# now we need to cut down this stack of envi varibales so that none are strongly autocorrelated with eachother

# two ways to look at this: pairwise comps or VIF scores (see Guisan)


# pairs(envi.global)  #this takes a while... don't need to run every time
# this does simple pairwise comparisons
# if we want to remove pairs with correlations of 0.85 or higher:
#  SST.max&SST.min, PHOmax & Phomin, and DAmax &DAmin all show strong autocorrelations
# there is no agreed upon limit for how correlated is too correlated
# martinez used 0.85, many use 0.8 or 0.7


#Pairs doesn't take into account complex multi-colinearity
# but VIF does 
# VIF = Variance Inflation Factor
# values of 5-10 best, up to 20 ok, above 20=bad, strong multicorelation

# vif() just returns the VIF values for each var, again want ideally less than 10 (or I'd take 20)

# vif(envi.global)

# results of prev line:
# > vif(envi.global)
# Variables        VIF
# # 1 e08.SST.max 358.907176
# # 2 e09.SST.min 576.962232
# # 3  e10.SST.ra 119.018796
# # 4  e07.SAL.me   1.347877
# # 5 e03.NIT.max   5.449913
# # 6 e04.NIT.min   8.015814
# # 7 e05.PHO.max  10.254266
# 8 e06.PHO.min  12.291757


# temp is the worst offender



# vifstep will go through and stepwise remove problem vars until all VIFs are below 10
# 
# vifstep(envi.global)
# 
# 
# #results from global vifstep:
# > vifstep(envi.global)
# 2 variables from the 8 input variables have collinearity problem: 
#   
#   e09.SST.min e06.PHO.min 
# 
# After excluding the collinear variables, the linear correlation coefficients ranges between: 
#   min correlation ( e05.PHO.max ~ e07.SAL.me ):  -0.0499544 
# max correlation ( e04.NIT.min ~ e03.NIT.max ):  0.8052716 
# 
# VIFs of the remained variables  
#   Variables      VIF
# 1 e08.SST.max 2.462531
# 2  e10.SST.ra 1.513469
# 3  e07.SAL.me 1.349815
# 4 e03.NIT.max 3.492575
# 5 e04.NIT.min 3.566174
# 6 e05.PHO.max 3.138695

#VIFstep shows SSTmin and Phomin having the worst correlations


# VIFstep suggests removing SSTmin, but based on Brezo's feedback I'm going to take out range instead, leaving in min. See what that does.

# for now leaving in Pho Min


# envi.global.cut1 <- stack(c(SST.max,
#                        SST.min,
#                        # SST.ran,
#                        SAL.men,
#                        NIT.max,
#                        NIT.min,
#                        PHO.max,
#                        PHO.min))
#                        # DA.max,
#                        # DA.min))
# 


# see if that worked to bring down the collinearity scores:

# vif(envi.global.cut1)

# > vif(envi.global.cut1)
# Variables       VIF
# 1 e08.SST.max  8.282085
# 2 e09.SST.min  7.097035
# 3  e07.SAL.me  1.283647
# 4 e03.NIT.max  4.369547
# 5 e04.NIT.min  7.592299
# 6 e05.PHO.max  9.266834
# 7 e06.PHO.min 12.037730



# ok this actually looks really good
# without SST range, the VIFs for everything else drop down a lot
# PHO min is still above 10, but very close (some authors say 20, so this is fine)
# vifstep still recommends removing pho.min, but that's ok. Keeping it for now. 

# using these 7 variables for the first iteration of the models
```

```{r model present}

#choosing 7 starting variables based on VIF autocorrelation tests
# see above


##### model 1

envi.global.1 <- stack(c(SST.max,
                       SST.min,
                       # SST.ran,
                       SAL.men,
                       NIT.max,
                       NIT.min,
                       PHO.max,
                       PHO.min))
                       # DA.max,
                       # DA.min))

codi.max.1 <- maxent(x = envi.global.1,
                         p = occur.codi,
                         path="CODI/Maxent outputs/codi.max.1", 
                         args=c('linear=TRUE',        # just linear and quadratic features, 
                                'quadratic=TRUE',     # others are too complicated and lead to overfitting
                                'product=FALSE', 
                                'threshold=FALSE', 
                                'hinge=FALSE',      #keep hinge off, response curves look terrible
                                'randomtestpoints=30',  #like to have 70% training and 30% testing
                                'responsecurves=TRUE',
                                'jackknife=TRUE'))
# this is pretty slow becasue it's such a large area.


# looks pretty good, high AUCs



##### Model 2 

# removing: NIT max & min
 
envi.global.2 <- stack(c(SST.max,
                       SST.min,
                       # SST.ran,
                       SAL.men,
                       # NIT.max,
                       # NIT.min,
                       PHO.max,
                       PHO.min))
                       # DA.max,
                       # DA.min))

codi.max.2 <- maxent(x = envi.global.2,
                         p = occur.codi,
                         path="CODI/Maxent outputs/codi.max.2", 
                         args=c('linear=TRUE',       
                                'quadratic=TRUE',    
                                'product=FALSE', 
                                'threshold=FALSE', 
                                'hinge=FALSE',     
                                'randomtestpoints=30',  
                                'responsecurves=TRUE',
                                'jackknife=TRUE'))


#### model 3

# removing PHO min only

envi.global.3 <- stack(c(SST.max,
                       SST.min,
                       # SST.ran,
                       SAL.men,
                       # NIT.max,
                       # NIT.min,
                       PHO.max))
                       # PHO.min))
                       # DA.max,
                       # DA.min))

codi.max.3 <- maxent(x = envi.global.3,
                         p = occur.codi,
                         path="CODI/Maxent outputs/codi.max.3", 
                         args=c('linear=TRUE',       
                                'quadratic=TRUE',    
                                'product=FALSE', 
                                'threshold=FALSE', 
                                'hinge=FALSE',     
                                'randomtestpoints=30',  
                                'responsecurves=TRUE',
                                'jackknife=TRUE'))




#### model 4

# removing SAL mean

envi.global.4 <- stack(c(SST.max,
                       SST.min,
                       # SST.ran,
                       # SAL.men,
                       # NIT.max,
                       # NIT.min,
                       PHO.max))
                       # PHO.min))
                       # DA.max,
                       # DA.min))

codi.max.4 <- maxent(x = envi.global.4,
                         p = occur.codi,
                         path="CODI/Maxent outputs/codi.max.4", 
                         args=c('linear=TRUE',       
                                'quadratic=TRUE',    
                                'product=FALSE', 
                                'threshold=FALSE', 
                                'hinge=FALSE',     
                                'randomtestpoints=30',  
                                'responsecurves=TRUE',
                                'jackknife=TRUE'))




#### model FINAL

# copy-paste of Model 3 - putting Salinity back in, removing it casued AUC to start declining

envi.global.final <- stack(c(SST.max,
                       SST.min,
                       # SST.ran,
                       SAL.men,
                       # NIT.max,
                       # NIT.min,
                       PHO.max))
                       # PHO.min))
                       # DA.max,
                       # DA.min))

codi.max.final <- maxent(x = envi.global.final,
                         p = occur.codi,
                         path="CODI/Maxent outputs/codi.max.final", 
                         args=c('linear=TRUE',       
                                'quadratic=TRUE',    
                                'product=FALSE', 
                                'threshold=FALSE', 
                                'hinge=FALSE',     
                                'randomtestpoints=30',  
                                'responsecurves=TRUE',
                                'jackknife=TRUE'))




```


```{r model present 4.1}


# redoing a few things from Brezo's comments, April 2022
# basically, using different criteria to choose variables for final model 
# Percent Contribution or Permutation Importance need to be at least 20% (and/or response curves must make sense)

# doing this into new 4.1 folder

# there's a bit of stochasticity in the models, so every time you run it the values are slightly different. 
# for variables on the cusp of that 20% cutoff, sometimes they're above and sometimes below.
# we could run it 10 times and take the average?

#running "full" model 5 times:


envi.global.1 <- stack(c(SST.max,
                       SST.min,
                       # SST.ran,
                       SAL.men,
                       NIT.max,
                       NIT.min,
                       PHO.max,
                       PHO.min))
                       # DA.max,
                       # DA.min))

codi.max.1 <- maxent(x = envi.global.1,
                         p = occur.codi,
                         path="CODI/Maxent outputs 4.1/codi.max.1", 
                         args=c('linear=TRUE',        # just linear and quadratic features, 
                                'quadratic=TRUE',     # others are too complicated and lead to overfitting
                                'product=FALSE', 
                                'threshold=FALSE', 
                                'hinge=FALSE',      #keep hinge off, response curves look terrible
                                'randomtestpoints=30',  #like to have 70% training and 30% testing
                                'responsecurves=TRUE',
                                'jackknife=TRUE'))

codi.max.2 <- maxent(x = envi.global.1,
                         p = occur.codi,
                         path="CODI/Maxent outputs 4.1/codi.max.2", 
                         args=c('linear=TRUE',       
                                'quadratic=TRUE',    
                                'product=FALSE', 
                                'threshold=FALSE', 
                                'hinge=FALSE',     
                                'randomtestpoints=30', 
                                'responsecurves=TRUE',
                                'jackknife=TRUE'))

codi.max.3 <- maxent(x = envi.global.1,
                         p = occur.codi,
                         path="CODI/Maxent outputs 4.1/codi.max.3", 
                         args=c('linear=TRUE',       
                                'quadratic=TRUE',    
                                'product=FALSE', 
                                'threshold=FALSE', 
                                'hinge=FALSE',     
                                'randomtestpoints=30', 
                                'responsecurves=TRUE',
                                'jackknife=TRUE'))


codi.max.4 <- maxent(x = envi.global.1,
                         p = occur.codi,
                         path="CODI/Maxent outputs 4.1/codi.max.4", 
                         args=c('linear=TRUE',       
                                'quadratic=TRUE',    
                                'product=FALSE', 
                                'threshold=FALSE', 
                                'hinge=FALSE',     
                                'randomtestpoints=30', 
                                'responsecurves=TRUE',
                                'jackknife=TRUE'))


codi.max.5 <- maxent(x = envi.global.1,
                         p = occur.codi,
                         path="CODI/Maxent outputs 4.1/codi.max.5", 
                         args=c('linear=TRUE',       
                                'quadratic=TRUE',    
                                'product=FALSE', 
                                'threshold=FALSE', 
                                'hinge=FALSE',     
                                'randomtestpoints=30', 
                                'responsecurves=TRUE',
                                'jackknife=TRUE'))


#################################
### Final Models

### Remove variables with <20% percent contribution OR permutation importance


envi.global.final <- stack(c(SST.max,
                       SST.min,
                       # SST.ran,
                       # SAL.men,
                       # NIT.max,
                       # NIT.min,
                       PHO.max))
                       # PHO.min))
                       # DA.max,
                       # DA.min))

codi.max.final.1 <- maxent(x = envi.global.final,
                         p = occur.codi,
                         path="CODI/Maxent outputs 4.1/codi.max.final.1", 
                         args=c('linear=TRUE',       
                                'quadratic=TRUE',    
                                'product=FALSE', 
                                'threshold=FALSE', 
                                'hinge=FALSE',     
                                'randomtestpoints=30', 
                                'responsecurves=TRUE',
                                'jackknife=TRUE'))




```






```{r project present}

#project model onto global scale
predict.codi.global <- predict(codi.max.final, envi.global.final)

ggplot() +
  layer_spatial(predict.codi.global)

#save as raster for plotting in arcmap
writeRaster(predict.codi.global, 
            filename = 'CODI/predict.caul.global.full.asc',
            format='ascii')

## cut raster based on threshold value
## threshold: equal test sensitivity and specificity ##


predict.codi.thresh <- predict.codi.global
predict.codi.thresh[predict.codi.thresh < 0.330] <- NA # this value changes by species

ggplot() +
  layer_spatial(predict.codi.thresh)

writeRaster(predict.codi.thresh,
            filename = 'CODI/predict.caul.thresh.asc',
            formatt= 'ascii')



```


```{r arcmap notes 1}

# happening over in ArcMap - just want to get everything written down in one place

# import occurrence records (same file as used here)
# import global predicted suitability raster (predict.____.global.full)
# import trimmed suitability to threshold raster (predict.____.thresh) This is H (highly suitable area)


#### need to make raster for currently occupied areas
## ( i did do this previously, but they were messy and I've changed some points - redoing)
# use buffer tool on occurrence points
# 100km, full, round ends, planar
# 
# use blank int poly shape that represents full global extent of data to clip occurrences
# remove areas outside test rasters / on land from occur raster
# This is O, occupied area
# 

#### Next, want to create shape for Vulnerable area V
# H - O = V
# but need to make stuff into polygons first (can't trim from orig asc rasters)

# To get polygon from orig raster:
## Use INT tool on orig asc (Toolbox Spatial Analysis - Math)
## Use Raster to Polygon tool (toolbox Conversions - from raster)
# now you have a polygon of the original shape, with no values.
# ready for trimming


# Make polygons for H and O
# use Erase tool (analysis - overlay) 
## Input = H; Erase = O

# now you have a V polygon


### Great, now just need to calculate areas of all of these polygons
### for now, just going to do some written descriptions





```


```{r CC envi data}
# need to bring in environmental layers for SST max and min under RCP45 scenario

# read in files
SST.max.RCP45 <- raster('Environmental Data/RCP45/e08.SST.max.asc')
SST.min.RCP45 <- raster('Environmental Data/RCP45/e09.SST.min.asc')

# trim to same extents as other vars
SST.max.RCP45 <- crop(SST.max.RCP45, ext.global.trim)
SST.min.RCP45 <- crop(SST.min.RCP45, ext.global.trim)


```


```{r CC models}

# models only for RCP45 scenarios
# take final model, predict onto raster stack representing future conditions
# future conditions stack includes future temps, but present other vars

## adjust the variables here to reflect vars remaining in full Final models above

envi.global.cc <- stack(c(SST.max.RCP45,
                       SST.min.RCP45,
                       # SST.ran,
                       SAL.men,
                       # NIT.max,
                       # NIT.min,
                       PHO.max))
                       # PHO.min))
                       # DA.max,
                       # DA.min))



## predict final model onto future stack

predict.codi.cc <- predict(codi.max.final, envi.global.cc)


ggplot() +
  layer_spatial(predict.codi.cc)

#save as raster for plotting in arcmap
writeRaster(predict.codi.cc, 
            filename = 'CODI/predict.codi.cc.full.2.asc',
            format='ascii')


## cut raster based on threshold value
## threshold: equal test sensitivity and specificity 

predict.codi.thresh.cc <- predict.codi.cc
predict.codi.thresh.cc[predict.codi.thresh.cc < 0.330] <- NA # this value changes by species

ggplot() +
  layer_spatial(predict.codi.thresh.cc)

writeRaster(predict.codi.thresh.cc,
            filename = 'CODI/predict.codi.thresh.cc.asc',
            formatt= 'ascii',
            overwrite = TRUE)
```


```{r ArcMap notes 2}

# next steps are again in ArcMap

# bring in predict.species.thresh.cc raster (showing high suitable habitat in future conditition)
# convert that raster to polygon (orig -> int -> polygon)
# Name polygon: Species High Suit CC (H-CC)

# next, want to make polygons showing gains and losses
# H-CC relative to H original (NOT V - want habitat in general, not just vulnerable area)

# Using Erase Tool
# G = Gain = H-CC - H  (color green)
# L = Loss = H - H-CC  (color purple)



### finished map
# has the following:

# Occurrence Points (dots)
# O = Occupied = buffer around points = color light blue
# H = Highly Suitable = value > threshold = color Yellow
# V = Vulnerable = H - O = Color Red
# H-CC = Highly Suitable under Future Climate Change conditions = color Pink
# G = Gain = H-CC - H = color Green
# L = Loss = H - H-CC = color Purple


# (Gain and loss are relative to H not V since we want the total habitat change, not just vulnerable areas)



```


```{r ArcMap notes 3}
# how to calculate areas of polygons
# not biologically really meaningful, but the relative numbers are important

# for EACH polygon:

# open attribute table
# Add Field - name it area
# right click on area field, Calculate Geometry
# choose area, set units to km2
# once calculated, right click on area field, Statistics
# stats output includes SUM - copy paste this into table or wherever you need it


```


```{r maps}

world <- ne_countries(scale='medium', returnclass='sf')

theme_map <- theme(panel.background = element_rect(color = 'black', fill = 'lightblue'),
                   panel.border = element_rect(color = 'black', fill=NA))

# Occurrence Points

ggplot(data=world) +
  geom_sf(color='black', fill='gray') + 
  geom_point(data=occur.codi, aes(x=longitude, y=latitude), 
             color='red', fill='red', size=1)+
  labs(caption = 'Figure 1: Global occurrence records of Codium fragile subsp. fragile.',
      x = '', y = '') +
  coord_sf(expand = F) +
  theme_map


### Present Vulnerable and Occupied areas

codi.occur.shp <- read_sf("ArcMap/CODI Arc/CODI R/CODI_Occur.shp")
codi.vuln.shp <- read_sf("ArcMap/CODI Arc/CODI R/CODI_Vuln.shp")


ggplot() +
  geom_sf(data = world, color = "black", fill = "gray") +
  geom_sf(data = codi.occur.shp, color = 'black', fill = ' black') +
  geom_sf(data = codi.vuln.shp, color = 'black', fill = ' red') +
  coord_sf(xlim = c(-75.0, -60.0), ylim=c(39.0, 46.0), expand=F) + #Gulf of Maine
  theme_map


### Future, gains and losses

codi.high.cc.shp <- read_sf("ArcMap/CODI Arc/CODI R/CODI_high_cc.shp")
codi.gain.shp <- read_sf("ArcMap/CODI Arc/CODI R/CODI_gain.shp")
codi.loss.shp <- read_sf("ArcMap/CODI Arc/CODI R/CODI_loss.shp")


ggplot() +
  geom_sf(data = world, color = "black", fill = "gray") +
  geom_sf(data = codi.high.cc.shp, color = 'black', fill = 'orange') +
  geom_sf(data = codi.gain.shp, color = 'black', fill = 'green') +
  geom_sf(data = codi.loss.shp, color = 'black', fill = 'purple') +
  coord_sf(xlim = c(-144.0,-110.0), ylim=c(29.0,58.0)) +    #West Coast
 # coord_sf(xlim = c(-75.0, -60.0), ylim=c(39.0, 46.0), expand=F) + #Gulf of Maine
  theme_map


```