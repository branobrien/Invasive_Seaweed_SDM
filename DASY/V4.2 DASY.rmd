---
title: "DASY"
author: "Brandon O'Brien"
date: "1/21/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = '~/Documents/Algae Lab UNH/Species Distribution Modeling/V4 - Jan 2022')
```

```{r library}

setwd("~/Documents/Algae Lab UNH/Species Distribution Modeling/V4 - Jan 2022")

library(ggplot2)
library(ggspatial)
library(sf)
library(rnaturalearth)
library(rnaturalearthdata)
library(raster)
library(dismo)
library(rJava)
library(usdm)
library(terra)
library(ggpubr)


```

```{r occur data}

# note - maxent function won't take the raw occurrence data, it wants a dataframe with only 2 columns.
# Also, needs to be in Long, Latt order or else it messes up.

occur.dasy <- read.csv('DASY/DASY Occur/Dasy Maxent V4.csv', header=T)
head(occur.dasy)
occur.dasy <- as.data.frame(occur.dasy)




```

```{r occur maps}

world <- ne_countries(scale='medium', returnclass='sf')


theme_map <- theme(panel.background = element_rect(color = 'black', fill = 'lightblue'),
                   panel.border = element_rect(color = 'black', fill=NA))
# global
ggplot(data=world) +
  geom_sf(color='black', fill='gray') + 
  geom_point(data=occur.dasy, aes(x=longitude, y=latitude), 
             color='red', fill='red', size=1)+
  labs(caption = 'Figure 1: Global occurrence records of Dasysiphonia japonica.',
      x = '', y = '') +
  coord_sf(expand = F) +
  theme_map



```


```{r envi data present-day}



# bringing in all environmental raster layers and getting them  trimmed and set up
# building raster'bricks' which are just collections of layers in same extent
# bricks are like stacks but apparently faster to work with.


## all envi layers
## these have been trimmed down in arcGIS to only the 100km belt along the coastlines

SST.max <- raster('Environmental Data/Clipped_100km/e08.SST.max.asc')
SST.min <- raster('Environmental Data/Clipped_100km/e09.SST.min.asc')
SST.ran <- raster('Environmental Data/Clipped_100km/e10.SST.ra.asc')
SAL.men <- raster('Environmental Data/Clipped_100km/e07.SAL.me.asc')
NIT.max <- raster('Environmental Data/Clipped_100km/e03.NIT.max.asc')
NIT.min <- raster('Environmental Data/Clipped_100km/e04.NIT.min.asc')
PHO.max <- raster('Environmental Data/Clipped_100km/e05.PHO.max.asc')
PHO.min <- raster('Environmental Data/Clipped_100km/e06.PHO.min.asc')
DA.max <- raster('Environmental Data/Clipped_100km/e01.DA.max.asc')
DA.min <- raster('Environmental Data/Clipped_100km/e02.DA.min.asc')



## NOtes
# Brezo had a lot to say about using DA as a variable - maybe I should just remove it from the start?
# Also said it may be worth it to include all the temp vars, max and min
# even if somewhat correlated

# need to trim off edges to make sure extents are all the same

ext.global.trim <- extent(c(-180,180,-65,65))

SST.max <- crop(SST.max, ext.global.trim)
SST.min <- crop(SST.min, ext.global.trim)
SST.ran <- crop(SST.ran, ext.global.trim)
SAL.men <- crop(SAL.men, ext.global.trim)
NIT.max <- crop(NIT.max, ext.global.trim)
NIT.min <- crop(NIT.min, ext.global.trim)
PHO.max <- crop(PHO.max, ext.global.trim)
PHO.min <- crop(PHO.min, ext.global.trim)
DA.max <- crop(DA.max, ext.global.trim)
DA.min <- crop(DA.min, ext.global.trim)

#not using DA on Brezo's suggestions

envi.global <- stack(c(SST.max,
                       SST.min,
                       SST.ran,
                       SAL.men,
                       NIT.max,
                       NIT.min,
                       PHO.max,
                       PHO.min))
                       # DA.max,
                       # DA.min))

```

```{r collinearity tests}

## this entire section is #d out since I don't need to rerun it during knitting (analysis done though)

# now we need to cut down this stack of envi varibales so that none are strongly autocorrelated with eachother

# two ways to look at this: pairwise comps or VIF scores (see Guisan)


# pairs(envi.global)  #this takes a while... don't need to run every time
# this does simple pairwise comparisons
# if we want to remove pairs with correlations of 0.85 or higher:
#  SST.max&SST.min, PHOmax & Phomin, and DAmax &DAmin all show strong autocorrelations
# there is no agreed upon limit for how correlated is too correlated
# martinez used 0.85, many use 0.8 or 0.7


#Pairs doesn't take into account complex multi-colinearity
# but VIF does 
# VIF = Variance Inflation Factor
# values of 5-10 best, up to 20 ok, above 20=bad, strong multicorelation

# vif() just returns the VIF values for each var, again want ideally less than 10 (or I'd take 20)

# vif(envi.global)

# results of prev line:
# > vif(envi.global)
# Variables        VIF
# # 1 e08.SST.max 358.907176
# # 2 e09.SST.min 576.962232
# # 3  e10.SST.ra 119.018796
# # 4  e07.SAL.me   1.347877
# # 5 e03.NIT.max   5.449913
# # 6 e04.NIT.min   8.015814
# # 7 e05.PHO.max  10.254266
# 8 e06.PHO.min  12.291757


# temp is the worst offender



# vifstep will go through and stepwise remove problem vars until all VIFs are below 10
# 
# vifstep(envi.global)
# 
# 
# #results from global vifstep:
# > vifstep(envi.global)
# 2 variables from the 8 input variables have collinearity problem: 
#   
#   e09.SST.min e06.PHO.min 
# 
# After excluding the collinear variables, the linear correlation coefficients ranges between: 
#   min correlation ( e05.PHO.max ~ e07.SAL.me ):  -0.0499544 
# max correlation ( e04.NIT.min ~ e03.NIT.max ):  0.8052716 
# 
# VIFs of the remained variables  
#   Variables      VIF
# 1 e08.SST.max 2.462531
# 2  e10.SST.ra 1.513469
# 3  e07.SAL.me 1.349815
# 4 e03.NIT.max 3.492575
# 5 e04.NIT.min 3.566174
# 6 e05.PHO.max 3.138695

#VIFstep shows SSTmin and Phomin having the worst correlations


# VIFstep suggests removing SSTmin, but based on Brezo's feedback I'm going to take out range instead, leaving in min. See what that does.

# for now leaving in Pho Min


# envi.global.cut1 <- stack(c(SST.max,
#                        SST.min,
#                        # SST.ran,
#                        SAL.men,
#                        NIT.max,
#                        NIT.min,
#                        PHO.max,
#                        PHO.min))
#                        # DA.max,
#                        # DA.min))
# 


# see if that worked to bring down the collinearity scores:

# vif(envi.global.cut1)

# > vif(envi.global.cut1)
# Variables       VIF
# 1 e08.SST.max  8.282085
# 2 e09.SST.min  7.097035
# 3  e07.SAL.me  1.283647
# 4 e03.NIT.max  4.369547
# 5 e04.NIT.min  7.592299
# 6 e05.PHO.max  9.266834
# 7 e06.PHO.min 12.037730



# ok this actually looks really good
# without SST range, the VIFs for everything else drop down a lot
# PHO min is still above 10, but very close (some authors say 20, so this is fine)
# vifstep still recommends removing pho.min, but that's ok. Keeping it for now. 

# using these 7 variables for the first iteration of the models
```


```{r models present-day}

envi.global.1 <- stack(c(SST.max,
                       SST.min,
                       # SST.ran,
                       SAL.men,
                       NIT.max,
                       NIT.min,
                       PHO.max,
                       PHO.min))
                       # DA.max,
                       # DA.min))

dasy.max.1 <- maxent(x = envi.global.1,
                         p = occur.dasy,
                         path="DASY/Maxent outputs/dasy.max.1", 
                         args=c('linear=TRUE',        # just linear and quadratic features, 
                                'quadratic=TRUE',     # others are too complicated and lead to overfitting
                                'product=FALSE', 
                                'threshold=FALSE', 
                                'hinge=FALSE',      #keep hinge off, response curves look terrible
                                'randomtestpoints=30',  #like to have 70% training and 30% testing
                                'responsecurves=TRUE',
                                'jackknife=TRUE'))
# this is pretty slow becasue it's such a large area.



##### model 2

#removing: NIT min, NIT max, PHO max


envi.global.2 <- stack(c(SST.max,
                       SST.min,
                       # SST.ran,
                       SAL.men,
                       # NIT.max,
                       # NIT.min,
                       # PHO.max,
                       PHO.min))
                       # DA.max,
                       # DA.min))

dasy.max.2 <- maxent(x = envi.global.2,
                         p = occur.dasy,
                         path="DASY/Maxent outputs/dasy.max.2", 
                         args=c('linear=TRUE',         
                                'quadratic=TRUE',     
                                'product=FALSE', 
                                'threshold=FALSE', 
                                'hinge=FALSE',      
                                'randomtestpoints=30',  
                                'responsecurves=TRUE',
                                'jackknife=TRUE'))


# keeping in SST max again even though prediction is low


### model Final
# copy-pasterd from Model 2
# SST min, SAL mean, PHO min all have 10+ % contrib, keeping SST Max in as well (even though is contrib very little)

envi.global.final <- stack(c(SST.max,
                       SST.min,
                       # SST.ran,
                       SAL.men,
                       # NIT.max,
                       # NIT.min,
                       # PHO.max,
                       PHO.min))
                       # DA.max,
                       # DA.min))

dasy.max.final <- maxent(x = envi.global.final,
                         p = occur.dasy,
                         path="DASY/Maxent outputs/dasy.max.final", 
                         args=c('linear=TRUE',         
                                'quadratic=TRUE',     
                                'product=FALSE', 
                                'threshold=FALSE', 
                                'hinge=FALSE',      
                                'randomtestpoints=30',  
                                'responsecurves=TRUE',
                                'jackknife=TRUE'))






```








```{r project present}

#project model onto global scale
predict.dasy.global <- predict(dasy.max.final, envi.global.final)

ggplot() +
  layer_spatial(predict.dasy.global)

#save as raster for plotting in arcmap
writeRaster(predict.dasy.global, 
            filename = 'DASY/predict.dasy.global.full.asc',
            format='ascii')

## cut raster based on threshold value
## threshold: equal test sensitivity and specificity =  ##


predict.dasy.thresh <- predict.dasy.global
predict.dasy.thresh[predict.dasy.thresh < 0.417] <- NA # this value changes by species

ggplot() +
  layer_spatial(predict.dasy.thresh)

writeRaster(predict.dasy.thresh,
            filename = 'DASY/predict.dasy.thresh.asc',
            formatt= 'ascii')



```


```{r arcmap notes 1}

# happening over in ArcMap - just want to get everything written down in one place

# import occurrence records (same file as used here)
# import global predicted suitability raster (predict.____.global.full)
# import trimmed suitability to threshold raster (predict.____.thresh) This is H (highly suitable area)


#### need to make raster for currently occupied areas
## ( i did do this previously, but they were messy and I've changed some points - redoing)
# use buffer tool on occurrence points
# 100km, full, round ends, planar
# 
# use blank int poly shape that represents full global extent of data to clip occurrences
# remove areas outside test rasters / on land from occur raster
# This is O, occupied area
# 

#### Next, want to create shape for Vulnerable area V
# H - O = V
# but need to make stuff into polygons first (can't trim from orig asc rasters)

# To get polygon from orig raster:
## Use INT tool on orig asc (Toolbox Spatial Analysis - Math)
## Use Raster to Polygon tool (toolbox Conversions - from raster)
# now you have a polygon of the original shape, with no values.
# ready for trimming


# Make polygons for H and O
# use Erase tool (analysis - overlay) 
## Input = H; Erase = O

# now you have a V polygon


### Great, now just need to calculate areas of all of these polygons
### for now, just going to do some written descriptions





```




```{r CC envi data}
# need to bring in environmental layers for SST max and min under RCP45 scenario

# read in files
SST.max.RCP45 <- raster('Environmental Data/RCP45/e08.SST.max.asc')
SST.min.RCP45 <- raster('Environmental Data/RCP45/e09.SST.min.asc')

# trim to same extents as other vars
SST.max.RCP45 <- crop(SST.max.RCP45, ext.global.trim)
SST.min.RCP45 <- crop(SST.min.RCP45, ext.global.trim)


```


```{r CC models}

# models only for RCP45 scenarios
# take final model, predict onto raster stack representing future conditions
# future conditions stack includes future temps, but present other vars

## adjust the variables here to reflect vars remaining in full Final models above

envi.global.cc <- stack(c(SST.max.RCP45,
                       SST.min.RCP45,
                       # SST.ran,
                       SAL.men,
                       # NIT.max,
                       # NIT.min,
                       # PHO.max,
                       PHO.min))
                       # DA.max,
                       # DA.min))



## predict final model onto future stack

predict.dasy.cc <- predict(dasy.max.final, envi.global.cc)


ggplot() +
  layer_spatial(predict.dasy.cc)

#save as raster for plotting in arcmap
writeRaster(predict.dasy.cc, 
            filename = 'DASY/predict.dasy.cc.full.asc',
            format='ascii',
            overwrite = TRUE)


## cut raster based on threshold value
## threshold: equal test sensitivity and specificity 

predict.dasy.thresh.cc <- predict.dasy.cc
predict.dasy.thresh.cc[predict.dasy.thresh.cc < 0.417] <- NA # this value changes by species

ggplot() +
  layer_spatial(predict.dasy.thresh.cc)

writeRaster(predict.dasy.thresh.cc,
            filename = 'DASY/predict.dasy.thresh.cc.asc',
            formatt= 'ascii',
            overwrite = TRUE)
```


```{r ArcMap notes 2}

# next steps are again in ArcMap

# bring in predict.species.thresh.cc raster (showing high suitable habitat in future conditition)
# convert that raster to polygon (orig -> int -> polygon)
# Name polygon: Species High Suit CC (H-CC)

# next, want to make polygons showing gains and losses
# H-CC relative to H original (NOT V - want habitat in general, not just vulnerable area)

# Using Erase Tool
# G = Gain = H-CC - H  (color green)
# L = Loss = H - H-CC  (color purple)



### finished map
# has the following:

# Occurrence Points (dots)
# O = Occupied = buffer around points = color light blue
# H = Highly Suitable = value > threshold = color Yellow
# V = Vulnerable = H - O = Color Red
# H-CC = Highly Suitable under Future Climate Change conditions = color Pink
# G = Gain = H-CC - H = color Green
# L = Loss = H - H-CC = color Purple


# (Gain and loss are relative to H not V since we want the total habitat change, not just vulnerable areas)



```



```{r ArcMap notes 3}
# how to calculate areas of polygons
# not biologically really meaningful, but the relative numbers are important

# for EACH polygon:

# open attribute table
# Add Field - name it area
# right click on area field, Calculate Geometry
# choose area, set units to km2
# once calculated, right click on area field, Statistics
# stats output includes SUM - copy paste this into table or wherever you need it


```


```{r maps}

world <- ne_countries(scale='medium', returnclass='sf')


theme_map <- theme(panel.background = element_rect(color = 'black', fill = 'lightblue'),
                   panel.border = element_rect(color = 'black', fill=NA))


## global occurrences

ggplot(data=world) +
  geom_sf(color='black', fill='gray') + 
  geom_point(data=occur.dasy, aes(x=longitude, y=latitude), 
             color='red', fill='red', size=1)+
  labs(caption = 'Figure 1: Global occurrence records of Dasysiphonia japonica.',
      x = '', y = '') +
  coord_sf(expand = F) +
  theme_map


## present occupied and vulnerable areas

dasy.occur.shp <- read_sf("C:/Users/Brandon/Documents/Algae Lab UNH/Species Distribution Modeling/V4 - Jan 2022/ArcMap/DASY Arc/DASY_R/DASY_occur.shp")
dasy.vuln.shp <- read_sf("C:/Users/Brandon/Documents/Algae Lab UNH/Species Distribution Modeling/V4 - Jan 2022/ArcMap/DASY Arc/DASY_R/DASY_vuln.shp")


ggplot() +
  geom_sf(data = world, color = "black", fill = "gray") +
  geom_sf(data = dasy.occur.shp, color = 'black', fill = ' black') +
  geom_sf(data = dasy.vuln.shp, color = 'black', fill = ' red') +
  coord_sf(xlim = c(-75.0, -60.0), ylim=c(39.0, 46.0), expand=F) + #Gulf of Maine
  theme_map



### Future gains and losses

dasy.high.cc.shp <- read_sf("C:/Users/Brandon/Documents/Algae Lab UNH/Species Distribution Modeling/V4 - Jan 2022/ArcMap/DASY Arc/DASY_R/DASY_high_cc.shp")
dasy.gain.shp <- read_sf("C:/Users/Brandon/Documents/Algae Lab UNH/Species Distribution Modeling/V4 - Jan 2022/ArcMap/DASY Arc/DASY_R/DASY_gain.shp")
dasy.loss.shp <- read_sf("C:/Users/Brandon/Documents/Algae Lab UNH/Species Distribution Modeling/V4 - Jan 2022/ArcMap/DASY Arc/DASY_R/DASY_loss.shp")
  
  
ggplot() +
  geom_sf(data = world, color = "black", fill = "gray") +
  geom_sf(data = dasy.high.cc.shp, color = 'black', fill = 'orange') +
  geom_sf(data = dasy.gain.shp, color = 'black', fill = 'green') +
  geom_sf(data = dasy.loss.shp, color = 'black', fill = 'purple') +
  theme_map



```


```{r maps2}


global.map <- ggplot(data=world) +
                geom_sf(color='black', fill='gray') + 
                geom_point(data=occur.dasy, aes(x=longitude, y=latitude), 
                  color='red', fill='red', size=1) +
                labs(x='', y='') +
                coord_sf(xlim = c(-180,180), ylim = c(70, -70), expand = F) +  #global
                theme_map


northeast.map <- ggplot(data=world) +
                geom_sf(color='black', fill='gray') + 
                geom_point(data=occur.dasy, aes(x=longitude, y=latitude), 
                  color='red', fill='red', size=1) +
                labs(x='', y='') +
                coord_sf(xlim = c(-75.0, -60.0), ylim=c(39.0, 46.0), expand = F) +  #New England
                theme_map


#europe map
europe.map <- ggplot(data=world) +
                geom_sf(color='black', fill='gray') + 
                geom_point(data=occur.dasy, aes(x=longitude, y=latitude), 
                  color='red', fill='red', size=1) +
                labs(x='', y='') +
                coord_sf(xlim = c(-20.0, 50.0), ylim=c(30.0,65.0), expand = F) +  #euro
                theme_map





global.map
northeast.map
europe.map



ggarrange(global.map, northeast.map, europe.map, labels = c("A", "B", "C"), 
          nrow = 2, ncol = 2)


ggarrange(global.map, 
          ggarrange(northeast.map,europe.map,
                    labels = c("B","C"), 
                    nrow = 1, ncol=2),
          labels=c("A"), 
          nrow=1, ncol=1)




```

```{r 4.1 opener}
## Making some changes based on feedback from Brezo April 2022
## mainly, only going to use variables with PC or PI of >20%
## so, need to redo the models (again)


```


```{r model present 4.1}


# redoing a few things from Brezo's comments, April 2022
# basically, using different criteria to choose variables for final model 
# Percent Contribution or Permutation Importance need to be at least 20% (and/or response curves must make sense)

# doing this into new 4.1 folder

# there's a bit of stochasticity in the models, so every time you run it the values are slightly different. 
# for variables on the cusp of that 20% cutoff, sometimes they're above and sometimes below.
# we could run it 10 times and take the average?

#running "full" model 5 times:

envi.global.1 <- stack(c(SST.max,
                       SST.min,
                       # SST.ran,
                       SAL.men,
                       NIT.max,
                       NIT.min,
                       PHO.max,
                       PHO.min))
                       # DA.max,
                       # DA.min))

dasy.max.1 <- maxent(x = envi.global.1,
                         p = occur.dasy,
                         path="DASY/Maxent outputs 4.1/dasy.max.1", 
                         args=c('linear=TRUE',        # just linear and quadratic features, 
                                'quadratic=TRUE',     # others are too complicated and lead to overfitting
                                'product=FALSE', 
                                'threshold=FALSE', 
                                'hinge=FALSE',      #keep hinge off, response curves look terrible
                                'randomtestpoints=30',  #like to have 70% training and 30% testing
                                'responsecurves=TRUE',
                                'jackknife=TRUE'))

dasy.max.2 <- maxent(x = envi.global.1,
                         p = occur.dasy,
                         path="DASY/Maxent outputs 4.1/dasy.max.2", 
                         args=c('linear=TRUE',        
                                'quadratic=TRUE',     
                                'product=FALSE', 
                                'threshold=FALSE', 
                                'hinge=FALSE',     
                                'randomtestpoints=30', 
                                'responsecurves=TRUE',
                                'jackknife=TRUE'))

dasy.max.3 <- maxent(x = envi.global.1,
                         p = occur.dasy,
                         path="DASY/Maxent outputs 4.1/dasy.max.3", 
                         args=c('linear=TRUE',        
                                'quadratic=TRUE',     
                                'product=FALSE', 
                                'threshold=FALSE', 
                                'hinge=FALSE',     
                                'randomtestpoints=30', 
                                'responsecurves=TRUE',
                                'jackknife=TRUE'))

dasy.max.4 <- maxent(x = envi.global.1,
                         p = occur.dasy,
                         path="DASY/Maxent outputs 4.1/dasy.max.4", 
                         args=c('linear=TRUE',        
                                'quadratic=TRUE',     
                                'product=FALSE', 
                                'threshold=FALSE', 
                                'hinge=FALSE',     
                                'randomtestpoints=30', 
                                'responsecurves=TRUE',
                                'jackknife=TRUE'))

dasy.max.5 <- maxent(x = envi.global.1,
                         p = occur.dasy,
                         path="DASY/Maxent outputs 4.1/dasy.max.5", 
                         args=c('linear=TRUE',        
                                'quadratic=TRUE',     
                                'product=FALSE', 
                                'threshold=FALSE', 
                                'hinge=FALSE',     
                                'randomtestpoints=30', 
                                'responsecurves=TRUE',
                                'jackknife=TRUE'))


#################################
### Final Models

### Remove variables with <20% percent contribution OR permutation importance



envi.global.final <- stack(c(#SST.max,
                       SST.min))
                       # SST.ran,
                       #SAL.men,
                       #NIT.max,
                       #NIT.min,
                       #PHO.max,
                       #PHO.min))
                       # DA.max,
                       # DA.min))



dasy.max.final.1 <- maxent(x = envi.global.final,
                         p = occur.dasy,
                         path="DASY/Maxent outputs 4.1/dasy.max.final.1", 
                         args=c('linear=TRUE',        
                                'quadratic=TRUE',     
                                'product=FALSE', 
                                'threshold=FALSE', 
                                'hinge=FALSE',     
                                'randomtestpoints=30', 
                                'responsecurves=TRUE',
                                'jackknife=TRUE'))
```

```{r 4.1 notes 1}

### ACtually, not using anything from the above chunk
# decided that I needed to use K-fold cross-valuidation
## which I can't figure out how to do in R
## so I did it in the desktop GUI instead
## have all the same outputs ready to go now
## back to messing with raster layers here in R and then in arc map


```

```{r 4.1 rasters present}

## following from notes above, I did the k-fold cross validation in the GUI on desktop
## now to continue with rest of the analysis
## GUI created the .asc files for me, just need to import those into here

# need to bring in the projected layer from maxent output
# cut it down based on the "highly suitable" threshold "equal test sens spec"
# bring that back into Arcmap and calculate areas

# bring in layer from maxent output
# 
# the k-fold runs k=5 models, then computes avg, max, min, etc for them
# we want the average value raster


predict.dasy.max <- raster("DASY/Maxent Outputs 4.1 GUI/dasy.max.final/dasysiphonia_japonica_avg.asc")

plot(predict.dasy.max)


#ok, got that, next need to cut the layer based on threshold found via maxent
# equal test sensitivity and specifity = 0.5887

predict.dasy.thresh <- predict.dasy.max
predict.dasy.thresh[predict.dasy.thresh < 0.5887] <- NA # threshold value, change to proper number (above)


ggplot() +
  layer_spatial(predict.dasy.thresh)

writeRaster(predict.dasy.thresh,
            filename = 'ArcMap/DASY Arc 4.1/predict.dasy.thresh.asc',
            format = 'ascii', 
            overwrite = T)




# happening over in ArcMap - just want to get everything written down in one place


# import trimmed suitability to threshold raster (predict.____.thresh) This is H (highly suitable area)
# to import properly, need to change projection from undefined to WGS84 - right click file from the catalog window; choose properties; edit Spatial Reference to WGS 84




#### Next, want to create shape for Vulnerable area V
# H - O = V
# but need to make stuff into polygons first (can't trim from orig asc rasters)

# To get polygon from orig raster:
## Use INT tool on orig asc (Toolbox Spatial Analysis - Math)
## Use Raster to Polygon tool (toolbox Conversions - from raster)
# now you have a polygon of the original shape, with no values.
# ready for trimming


# Make polygons for H and O
# use Erase tool (analysis - overlay) 
## Input = H; Erase = O

# now you have a V polygon


### Great, now just need to calculate areas of all of these polygons
## We'll calc areas later, after we have G and L polygons as well


```


```{r 4.1 future CC}

## again, doing this all in the GUI to keep it now consistent with the k-fold from earlier

## use future temp layers, but present other layers

## in GUI, for "projection layers", choose folder Envi Data - RCP45
## has all the layers appropriately renamed to match the present layers
## running again with k-fold crossvalidation, k=5
## and linear and product features only

## next, read in predicted raster avg values:


predict.dasy.rcp45 <- raster("DASY/Maxent Outputs 4.1 GUI/dasy.max.future/dasysiphonia_japonica_RCP45_avg.asc")



## cut to same threshold as before (equal test sens & spec)
## thresh = 0.5887

predict.dasy.rcp45.thresh <- predict.dasy.rcp45
predict.dasy.rcp45.thresh[predict.dasy.rcp45.thresh < 0.5887] <- NA 

writeRaster(predict.dasy.rcp45.thresh,
            filename = 'ArcMap/DASY Arc 4.1/predict.dasy.rcp45.thresh.asc',
            format = 'ascii', 
            overwrite = T)


### back over in Arcmap
## import this rcp45.thresh layer - represents highly suitable under future CC projections
## convert to polygon using same steps as above

## create Gain and Loss polygons
# H-CC relative to H original (NOT V - want habitat in general, not just vulnerable area)

# Using Erase Tool
# G = Gain = H-CC - H  (color green)
# L = Loss = H - H-CC  (color purple)


```




```{r sidebar}

# something is screwy here
# brought in H raster into ARcmap, and it looks totally different from previous iterations of the model (ie entire New England NOT suitable???)
# doesn't make logical sense
# which model is more correct???


# testing out rerunning this model in R
# can't do k-folds, but can do 70/30 split (just one run)
# in theory, R and GUI should produce same results.
# let's test that.

# with just SST.min, as determined by <20% method

# looking at present-day envi vars

# 1 - build model


envi.global.R <- stack(c(#SST.max,
                       SST.min))
                       # SST.ran,
                       # SAL.men,
                       # NIT.max,
                       # NIT.min,
                       # PHO.max,
                       # PHO.min))
                       # DA.max,
                       # DA.min))

dasy.max.R <- maxent(x = envi.global.R,
                         p = occur.dasy,
                         path="DASY/Maxent outputs 4.1 R/dasy.max.final", 
                         args=c('linear=TRUE',        # just linear and quadratic features, 
                                'quadratic=TRUE',     # others are too complicated and lead to overfitting
                                'product=FALSE', 
                                'threshold=FALSE', 
                                'hinge=FALSE',      #keep hinge off, response curves look terrible
                                'randomtestpoints=30',  #like to have 70% training and 30% testing
                                'responsecurves=TRUE',
                                'jackknife=TRUE'))



#project model onto global scale
predict.dasy.global.R <- predict(dasy.max.R, envi.global.R)


predict.dasy.thresh.R <- predict.dasy.global.R
predict.dasy.thresh.R[predict.dasy.thresh.R < 0.4687] <- NA # based on new R results



#save as raster for plotting in arcmap
writeRaster(predict.dasy.thresh.R, 
            filename = 'ArcMap/DASY Arc 4.1/predict.dasy.thresh.R.asc',
            format='ascii',
            overwrite=T)




## yeah, the R results and the GUI results don't match at all. Something is up.
## not sure which one to trust?
## R results seem more reasonable/logical (at least here for DASY)

## I think I may just go back to what I was doing in R originally. 
## stick with the n times 70-30 split average
## 



## from feedback from Brezo:
## installed java updates and redownloaded most recent version of Jar version, testing again:

predict.dasy.rcp45 <- raster("DASY/Maxent Outputs 4.1 GUI/dasy.max.test/dasysiphonia_japonica_fixed_avg.asc")

predict.dasy.rcp45.thresh <- predict.dasy.rcp45
predict.dasy.rcp45.thresh[predict.dasy.rcp45.thresh < 0.5887] <- NA 

setwd("~/Documents/Algae Lab UNH/Species Distribution Modeling/V4 - Jan 2022")

writeRaster(predict.dasy.rcp45.thresh,
            filename = 'ArcMap/DASYtest/predict.dasy.thresh.RCP45.GUI.test.asc',
            format = 'ascii', 
            overwrite = T)

## test #2

predict.dasy.test2 <- raster("DASY/Maxent Outputs 4.1 GUI/dasy.max.test/dasysiphonia_japonica_RCP45_avg.asc")

predict.dasy.rcp45.thresh.test2 <- predict.dasy.test2
predict.dasy.rcp45.thresh.test2[predict.dasy.rcp45.thresh.test2 < 0.5887] <- NA 

setwd("~/Documents/Algae Lab UNH/Species Distribution Modeling/V4 - Jan 2022")

writeRaster(predict.dasy.rcp45.thresh.test2,
            filename = 'ArcMap/DASYtest/predict.dasy.thresh.test2.asc',
            format = 'ascii', 
            overwrite = T)



```



```{r 4.2??? }

# frankly I'm pretty lost here.
# GUI now seems to produce the correct output - good
# predicted future maps are identical - bad
# 









```